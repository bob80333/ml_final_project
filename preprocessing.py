# This will handle all the data preprocessing
# I am still working on the details, but I know the general flow.
# some pertinent information about the data:
# 1. the original commonvoice audio is in mp3 format, which is slow to load, it needs to be converted to wav to match CVSS
# 2. the CVSS data is 24khz, which is generally good enough for speech, since nobody speaks even near 12khz, max freq represented in 24khz audio
# for reference, even if we want 4th harmonic tone within 12khz, 12/4 = 3khz, and that's about a G7! 
# I don't think anyone can even sing that high, let alone speak.
# so we should convert CV to 24khz to match CVSS
# 3. the CVSS data doesn't have too much silence at the beginning or end, which makes sense since it was generated by a TTS model.
# However, to ensure similar silence levels, we should trim the CV data to match the CVSS data.
# We will use a VAD model (voice activity detection) to trim the audio.

# 4. we want the audio of 2 sentences in the different languages to match, to make training the model easier.
# the trimming should have helped, but to make sure of it, 
# we can use pyrubberband which is a library for time stretching audio.
# we should time stretch whichever audio is shorter to match the longer one for each pair.

# FUTURE EDIT:this will make the audio sound bad if there's a big gap in timing between the 2 languages
# but there shouldn't really be a big gap in timing, german and english were chosen due to 1. data scale (2nd largest pair) and 2. similarity between languages
# # also if the segment length is long enough relative to sample rate (e.g. segment of 32k for 24khz audio = ~1.3 seconds)
# then a little mismatch shouldn't matter
# the dataloader can just do a relative position crop: e.g. pick a start point from 0 to 1, then multiply by remaining length for each audio independently
# unless there is a huge mismatch this should work
# and huge mismatch means this whole modelling/approach is flawed anyway and we need to do much larger segments / whole sentence pairs



# 5. Finally we want to use the CVSS split across both CVSS and CV, since CV does not come presplit like CVSS does, 
# so this script should put CV into train, dev, test folders based on the CVSS split.

import argparse
import os
import subprocess
from pathlib import Path
import torch
from tqdm import tqdm
import random
import math

def rescale_speech_timesteps(timesteps, vad_sr, actual_sr):
    scaling_factor = actual_sr / vad_sr
    for segment in timesteps:
        segment['start'] = math.floor(segment['start'] * scaling_factor)
        segment['end'] = math.floor(segment['end'] * scaling_factor)

# silero-vad only supports 8khz or 16khz sample rate
VAD_SAMPLE_RATE = 16000

if __name__ == "__main__":
    # use argparse to get the paths to the data
    parser = argparse.ArgumentParser(description='Preprocess the data')
    parser.add_argument('--cvss', type=str, default='data/cvss', help='Path to the CVSS data')
    parser.add_argument('--cv', type=str, default='data/cv', help='Path to the CV data')
    parser.add_argument('--output', type=str, default='data/preprocessed', help='Path to the output folder')
    parser.add_argument('--size', type=int, default=0, help='Number of samples to use from CVSS, if 0 use all samples')
    args = parser.parse_args()

    print("Converting CV to wav and 24khz.")
    # 1 & 2. convert CV to wav and 24khz using ffmpeg command line tool
    # only convert files in CV that are already in CVSS, otherwise we don't need them
    files = [str(x.absolute()) for x in Path(args.cv).rglob('*.mp3')]
    
    # only filename of files in cvss
    cvss_filenames = [str(x.absolute()).split("/")[-1] for x in Path(args.cvss).rglob('*.wav')]
    if args.size > 0:
        cvss_filenames = set(random.sample(cvss_filenames, args.size))
    else:
        cvss_filenames = set(cvss_filenames)
    files_to_convert = []
    print("Finding files to convert.")
    for file in tqdm(files):
        filename = file.split("/")[-1].replace(".mp3", ".mp3.wav")
        if filename in cvss_filenames:
            files_to_convert.append(file)
    
    os.makedirs(f'{args.cv}/clips_wav', exist_ok=True)
    print("Converting files:")
    for file in tqdm(files_to_convert):
        subprocess.run(f'ffmpeg -i {file} -ar 24000 {file.replace("clips", "clips_wav").replace(".mp3", ".mp3.wav")} -hide_banner -loglevel error', shell=True)
    
    # 3. Use silero VAD to trim the audio
    
    print("Trimming audio based on speech.")
    files = [x.absolute() for x in Path(args.cv).rglob('*.wav')]
    
    model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',
                              model='silero_vad',
                              force_reload=True,
                              onnx=False)
    
    (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
    
    print("Processing CV data.")
    os.makedirs(f'{args.cv}/clips_wav_speechonly', exist_ok=True)
    # tqdm gives progress bar
    for file in tqdm(files):
        wav = read_audio(str(file), sampling_rate=VAD_SAMPLE_RATE)
        speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=VAD_SAMPLE_RATE)
        # save only the speech segments to a new directory
        rescale_speech_timesteps(speech_timestamps, VAD_SAMPLE_RATE, 24000)
        wav_fullres = read_audio(str(file), sampling_rate=24000)
        save_audio(str(file).replace("clips_wav", "clips_wav_speechonly"), collect_chunks(speech_timestamps, wav_fullres), sampling_rate=24000)
        
    # do it again for CVSS
    files = [x.absolute() for x in Path(args.cvss).rglob('*.wav')]
    files = [x for x in files if str(x).split("/")[-1] in cvss_filenames]
    
    os.makedirs(f'{args.cvss}/train_speechonly', exist_ok=True)
    os.makedirs(f'{args.cvss}/dev_speechonly', exist_ok=True)
    os.makedirs(f'{args.cvss}/test_speechonly', exist_ok=True)
    
    print("Processing CVSS data.")
    for file in tqdm(files):
        file = str(file)
        wav = read_audio(str(file), sampling_rate=VAD_SAMPLE_RATE)        
        wav_fullres = read_audio(str(file), sampling_rate=24000)
        speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=VAD_SAMPLE_RATE)
        # save only the speech segments to a new directory
        if "train" in file:
            file = file.replace("train", "train_speechonly")
        elif "dev" in file:
            file = file.replace("dev", "dev_speechonly")
        elif "test" in file:
            file = file.replace("test", "test_speechonly")
            
        rescale_speech_timesteps(speech_timestamps, VAD_SAMPLE_RATE, 24000)
        save_audio(file, collect_chunks(speech_timestamps, wav), sampling_rate=24000)
        
    
    # 4. use pyrubberband to time stretch the shorter audios in each pair to match the longer one
    # FUTURE EDIT: not doing this, see comments at top of file
    
    # 5. take the data and use the CVSS split to split the CV data into train, dev, test folders
    # also move all the data into new folders
    
    # make output directories
    
    # makedirs will make intermediate folders like args.output and args.output/train for us

    print("Moving processed data to output folders.")
    os.makedirs(f'{args.output}/train/german', exist_ok=True)
    os.makedirs(f'{args.output}/train/english', exist_ok=True)
    
    os.makedirs(f'{args.output}/dev/german', exist_ok=True)
    os.makedirs(f'{args.output}/dev/english', exist_ok=True)
    
    os.makedirs(f'{args.output}/test/german', exist_ok=True)
    os.makedirs(f'{args.output}/test/english', exist_ok=True)
    
    # move the CV data
    # get all train files of cvss, then change to CV paths
    train_files = [str(x.absolute()).replace(args.cvss, args.cv).replace("train_speechonly", "clips_wav_speechonly") for x in Path(f'{args.cvss}/train_speechonly').rglob('*.wav')]
    for file in train_files:
        subprocess.run(f'mv {file} {args.output}/train/german', shell=True)
        
    dev_files = [str(x.absolute()).replace(args.cvss, args.cv).replace("dev_speechonly", "clips_wav_speechonly") for x in Path(f'{args.cvss}/dev_speechonly').rglob('*.wav')]
    for file in dev_files:
        subprocess.run(f'mv {file} {args.output}/dev/german', shell=True)
        
    test_files = [str(x.absolute()).replace(args.cvss, args.cv).replace("test_speechonly", "clips_wav_speechonly") for x in Path(f'{args.cvss}/test_speechonly').rglob('*.wav')]
    for file in test_files:
        subprocess.run(f'mv {file} {args.output}/test/german', shell=True)
    
    
    # move the CVSS data
    subprocess.run(f'mv {args.cvss}/train_speechonly/*.wav {args.output}/train/english', shell=True)
    subprocess.run(f'mv {args.cvss}/dev_speechonly/*.wav {args.output}/dev/english', shell=True)
    subprocess.run(f'mv {args.cvss}/test_speechonly/*.wav {args.output}/test/english', shell=True)
    
    # 6. delete all the intermediate files creating during preprocessing
    
    # print("Deleting intermediate files.")
    # delete CVSS speechonly dirs
    subprocess.run(f'rm -rf {args.cvss}/train_speechonly', shell=True)
    subprocess.run(f'rm -rf {args.cvss}/dev_speechonly', shell=True)
    subprocess.run(f'rm -rf {args.cvss}/test_speechonly', shell=True)
    
    # delete CV wav and speechonly dirs
    subprocess.run(f'rm -rf {args.cv}/clips_wav', shell=True)
    subprocess.run(f'rm -rf {args.cv}/clips_wav_speechonly', shell=True)
    
    print("Done!")